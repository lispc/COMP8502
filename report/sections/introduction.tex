\section{Introduction}
Smartphones became a part of our daily lives and apart from the standard phone functionality, such as calls, SMS and MMS messages, we use them in many different scenarios: reading e-mails and replying to them, readings news, navigating on a map, browsing personal and corporate social networks, taking pictures and uploading them to cloud repositories, playing games, checking our bank account balances or even paying for small purchases using NFC. Many of these scenarios deal with our private data that could be possibly misused by a malicious application.

We can add any extra functionality to our smartphone simply by installing new applications, just like on our personal computers. This can be done via the official central repositories, such as Google Play  \cite{google_play} or iOS App Store, which try to verify that applications do not contain a malicious code. The situation is different with Android, the most popular smartphone platform with a 79\% share of the global market \cite{idc}. Android gives users a freedom to choose third party sites instead of Google Play as well as to install a manually downloaded apps. This freedom comes with a price of a greater vulnerability to malicious applications.

In this project, we collected a sample of 1840 Android applications (283 malware ones, and 1557 non-malware), implemented a tool to extract static features of each application, evaluated .... TODO

The rest of this report is organised as follows. Section 2 briefly overviews recent research that inspired this project. Section 3 describes the collected dataset, extracted features, and our experimental approach. Section 4 presents our results. Section 5 discusses limitations of this project and potential improvements. Section 6 summarises the overall report.

\section{Related Work}
Shabtai, Fledel, and Elovici \cite{shabtai_2010} collected 2285 Android apps from Google Play and tried to classify them as either games or tools due to a lack of malware samples. They extracted various static features of each application, used three filters to select them based on one of three measures (Chi-Square, Fisher Score, and Information Gain), and evaluated them on the following classifiers: Decision Tree, Naive Bayes, Bayesian Networks, PART, Boosted Bayesian Networks, Boosted Decision Tree, Random Forest, Voting Feature Intervals in Weka. Their best obtained result (an accuracy level of 0.918 and 0.172 FPR) was with Boosted Bayesian Networks and the top 800 features selected using Information Gain.

Kim et al. \cite{kim_malicious} collected 893 Android apps from Google Play and 110 malware apps. They extracted static features using Dedexer and AXMLPrinter tools (API calls in DEX files and permissions in Android manifest files), and used J48 Decision Tree from Weka to classify them. They obtained an accuracy of 0.773 and 0.173 FPR which was an improvement over a detection framework that only used permissions (with 0.609 FPR).

Sanz et al. \cite{sanz_2012} collected 820 Android apps from Google Play and tried to classify them into one of seven categories (Entertainment, Puzzle and brain games, Communication, Multimedia and video, Society, Productivity, Tools). They extracted static features of each application (used strings, and permissions) as well as information from Google Play (rating, number of ratings, and application size). They used four models from Weka: Decision Trees (Random Forests and J48), K-Nearest Neighbour (with k set to 1, 2 and 5), Bayesian Networks (using several structural learning algorithms: K2, Hill Climber, TAN, and Naive Bayes), SVM (SMO, and experimented with a polynomial kernel and a normalised polynomial kernel). Their best reported result was with Bayesian Networks using TAN where they obtained 0.93 of the Area Under the ROC Curve. 